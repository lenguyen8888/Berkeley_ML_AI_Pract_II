{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a data perspective, our task is to develop a predictive model to identify the key determinants that influence the price of used cars. We will analyze a dataset containing features such as year, manufacturer, model, condition, cylinders, fuel, odometer, transmission, drive, size, type, paint_color, and region, and aim to understand how these variables contribute to the price of the vehicles. Our goal is to leverage statistical and machine learning techniques to uncover relationships and patterns within the data that can inform our pricing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Get Familiar with the Dataset and Identify Quality Issues\n",
    "\n",
    "1. **Load the Data**\n",
    "    - Import the dataset into a data manipulation tool such as Python (using pandas) or R.\n",
    "    - Display the first few rows to get an initial sense of the data structure.\n",
    "\n",
    "2. **Data Overview**\n",
    "    - Check the dimensions of the dataset (number of rows and columns).\n",
    "    - Review column names and data types to ensure they align with the expected features.\n",
    "\n",
    "3. **Handle Missing Values**\n",
    "    - Identify columns with missing values and the proportion of missing data in each column.\n",
    "\n",
    "4. **Descriptive Statistics**\n",
    "    - Calculate summary statistics (mean, median, mode, standard deviation) for numerical columns to understand the distribution of the data.\n",
    "    - For categorical columns, count the occurrences of each category.\n",
    "\n",
    "5. **Data Cleaning**\n",
    "    - Remove or impute any rows with significant amounts of missing data or outliers.\n",
    "    - Standardize the formats of any categorical columns (e.g., ensure all `manufacturer` names are consistently formatted).\n",
    "\n",
    "6. **Identify Duplicates**\n",
    "    - Check for and remove any duplicate rows that may skew analysis.\n",
    "\n",
    "7. **Data Transformation**\n",
    "    - Convert categorical variables into numerical formats if necessary (e.g., using one-hot encoding).\n",
    "    - Normalize or scale numerical variables to ensure they are on a comparable scale for modeling.\n",
    "\n",
    "8. **Exploratory Data Analysis (EDA)**\n",
    "    - Create visualizations such as histograms, box plots, and scatter plots to identify trends, patterns, and outliers in the data.\n",
    "    - Analyze correlations between variables to identify potential drivers of the target variable (`price`).\n",
    "\n",
    "9. **Feature Engineering**\n",
    "    - Create new features that may be relevant, such as `car_age` (derived from `year`).\n",
    "    - Consider aggregating or combining features that might have a joint impact on the target variable.\n",
    "\n",
    "10. **Data Quality Check**\n",
    "    - Validate data consistency, ensuring no discrepancies or logical errors within the columns.\n",
    "    - Document any data quality issues and the steps taken to address them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/vehicles.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                  region  price  year manufacturer model  \\\n",
      "0  7222695916                prescott   6000   NaN          NaN   NaN   \n",
      "1  7218891961            fayetteville  11900   NaN          NaN   NaN   \n",
      "2  7221797935            florida keys  21000   NaN          NaN   NaN   \n",
      "3  7222270760  worcester / central MA   1500   NaN          NaN   NaN   \n",
      "4  7210384030              greensboro   4900   NaN          NaN   NaN   \n",
      "\n",
      "  condition cylinders fuel  odometer title_status transmission  VIN drive  \\\n",
      "0       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "1       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "2       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "3       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "4       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "\n",
      "  size type paint_color state  \n",
      "0  NaN  NaN         NaN    az  \n",
      "1  NaN  NaN         NaN    ar  \n",
      "2  NaN  NaN         NaN    fl  \n",
      "3  NaN  NaN         NaN    ma  \n",
      "4  NaN  NaN         NaN    nc  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426880 entries, 0 to 426879\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            426880 non-null  int64  \n",
      " 1   region        426880 non-null  object \n",
      " 2   price         426880 non-null  int64  \n",
      " 3   year          425675 non-null  float64\n",
      " 4   manufacturer  409234 non-null  object \n",
      " 5   model         421603 non-null  object \n",
      " 6   condition     252776 non-null  object \n",
      " 7   cylinders     249202 non-null  object \n",
      " 8   fuel          423867 non-null  object \n",
      " 9   odometer      422480 non-null  float64\n",
      " 10  title_status  418638 non-null  object \n",
      " 11  transmission  424324 non-null  object \n",
      " 12  VIN           265838 non-null  object \n",
      " 13  drive         296313 non-null  object \n",
      " 14  size          120519 non-null  object \n",
      " 15  type          334022 non-null  object \n",
      " 16  paint_color   296677 non-null  object \n",
      " 17  state         426880 non-null  object \n",
      "dtypes: float64(2), int64(2), object(14)\n",
      "memory usage: 58.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426880 entries, 0 to 426879\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   region        426880 non-null  object \n",
      " 1   price         426880 non-null  int64  \n",
      " 2   year          425675 non-null  float64\n",
      " 3   manufacturer  409234 non-null  object \n",
      " 4   model         421603 non-null  object \n",
      " 5   odometer      422480 non-null  float64\n",
      " 6   title_status  418638 non-null  object \n",
      " 7   transmission  424324 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 26.1+ MB\n",
      "before dropna None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 391154 entries, 27 to 426879\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   region        391154 non-null  object \n",
      " 1   price         391154 non-null  int64  \n",
      " 2   year          391154 non-null  float64\n",
      " 3   manufacturer  391154 non-null  object \n",
      " 4   model         391154 non-null  object \n",
      " 5   odometer      391154 non-null  float64\n",
      " 6   title_status  391154 non-null  object \n",
      " 7   transmission  391154 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 26.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# drop columns id, state since region is already in the data\n",
    "df.drop(columns=['id', 'state'], inplace=True)\n",
    "# drop columns with too few Non-Null count < 400000\n",
    "df.drop(columns=['condition', 'cylinders', 'fuel', 'VIN',\\\n",
    "                  'drive', 'size', 'type', 'paint_color'], inplace=True)\n",
    "print('before dropna', df.info())\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'car_age' feature\n",
    "df['car_age'] = 2025 - df['year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "num_features = ['odometer', 'car_age']\n",
    "cat_features = ['model', 'title_status', 'transmission', 'region']\n",
    "\n",
    "# Create transformers for numerical and categorical data\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_features),\n",
    "        ('cat', cat_transformer, cat_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable and features\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Linear Regression\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_reg = Ridge()\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "# Decision Tree Regression\n",
    "tree_reg = DecisionTreeRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 9834046.542543653\n",
      "Ridge Regression RMSE: 9775864.85015646\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation for Linear Regression\n",
    "linear_scores = cross_val_score(linear_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "linear_rmse_scores = np.sqrt(-linear_scores)\n",
    "print(\"Linear Regression RMSE:\", linear_rmse_scores.mean())\n",
    "\n",
    "# Perform cross-validation for Ridge Regression\n",
    "ridge_scores = cross_val_score(ridge_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_rmse_scores = np.sqrt(-ridge_scores)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse_scores.mean())\n",
    "\n",
    "# # Perform cross-validation for Decision Tree Regression\n",
    "# tree_scores = cross_val_score(tree_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "# tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "# print(\"Decision Tree Regression RMSE:\", tree_rmse_scores.mean())\n",
    "\n",
    "# # Perform cross-validation for Lasso Regression\n",
    "# lasso_scores = cross_val_score(lasso_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "# lasso_rmse_scores = np.sqrt(-lasso_scores)\n",
    "# print(\"Lasso Regression RMSE:\", lasso_rmse_scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ridge Parameters: {'alpha': 100}\n",
      "Best Ridge RMSE: 12596165.971528497\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression hyperparameter tuning\n",
    "ridge_param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "ridge_grid_search = GridSearchCV(ridge_reg, ridge_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid_search.fit(X_train, y_train)\n",
    "print(\"Best Ridge Parameters:\", ridge_grid_search.best_params_)\n",
    "print(\"Best Ridge RMSE:\", np.sqrt(-ridge_grid_search.best_score_))\n",
    "\n",
    "# # Decision Tree Regression hyperparameter tuning\n",
    "# tree_param_grid = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 10, 20]}\n",
    "# tree_grid_search = GridSearchCV(tree_reg, tree_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "# tree_grid_search.fit(X_train, y_train)\n",
    "# print(\"Best Decision Tree Parameters:\", tree_grid_search.best_params_)\n",
    "# print(\"Best Decision Tree RMSE:\", np.sqrt(-tree_grid_search.best_score_))\n",
    "\n",
    "# # Lasso Regression hyperparameter tuning\n",
    "# lasso_param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "# lasso_grid_search = GridSearchCV(lasso_reg, lasso_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "# lasso_grid_search.fit(X_train, y_train)\n",
    "# print(\"Best Lasso Parameters:\", lasso_grid_search.best_params_)\n",
    "# print(\"Best Lasso RMSE:\", np.sqrt(-lasso_grid_search.best_score_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Test RMSE: 4657010.1293513095\n",
      "Ridge Regression Test RMSE: 4452942.552492419\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation for Linear Regression\n",
    "linear_reg.fit(X_train, y_train)\n",
    "linear_predictions = linear_reg.predict(X_test)\n",
    "print(\"Linear Regression Test RMSE:\", np.sqrt(mean_squared_error(y_test, linear_predictions)))\n",
    "\n",
    "# Final evaluation for Ridge Regression\n",
    "best_ridge_reg = ridge_grid_search.best_estimator_\n",
    "best_ridge_reg.fit(X_train, y_train)\n",
    "ridge_predictions = best_ridge_reg.predict(X_test)\n",
    "print(\"Ridge Regression Test RMSE:\", np.sqrt(mean_squared_error(y_test, ridge_predictions)))\n",
    "\n",
    "# # Final evaluation for Decision Tree Regression\n",
    "# best_tree_reg = tree_grid_search.best_estimator_\n",
    "# best_tree_reg.fit(X_train, y_train)\n",
    "# tree_predictions = best_tree_reg.predict(X_test)\n",
    "# print(\"Decision Tree Regression Test RMSE:\", np.sqrt(mean_squared_error(y_test, tree_predictions)))\n",
    "\n",
    "# # Final evaluation for Lasso Regression\n",
    "# best_lasso_reg = lasso_grid_search.best_estimator_\n",
    "# best_lasso_reg.fit(X_train, y_train)\n",
    "# lasso_predictions = best_lasso_reg.predict(X_test)\n",
    "# print(\"Lasso Regression Test RMSE:\", np.sqrt(mean_squared_error(y_test, lasso_predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Feature   Coefficient  Absolute Coefficient\n",
      "3632                    model_benz s430  2.572334e+07          2.572334e+07\n",
      "3524                    model_benz e320  1.867571e+07          1.867571e+07\n",
      "9009       model_f350 super duty lariat  5.771308e+06          5.771308e+06\n",
      "20284                  region_frederick  4.490146e+06          4.490146e+06\n",
      "1651                      model_4runner  3.972500e+06          3.972500e+06\n",
      "...                                 ...           ...                   ...\n",
      "7508    model_expedition eddie bauer4x4  5.938823e-01          5.938823e-01\n",
      "9486             model_forester 2.5 awd -3.282600e-01          3.282600e-01\n",
      "10763                    model_hse spor  2.647360e-01          2.647360e-01\n",
      "5324   model_cooper hardtop hatchback f  2.510420e-01          2.510420e-01\n",
      "8952            model_f350 diesel dualy -1.167850e-01          1.167850e-01\n",
      "\n",
      "[20571 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming linear_reg or ridge_reg is the trained model\n",
    "import pandas as pd\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = preprocessor.transformers_[0][2] + list(preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_features))\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = best_ridge_reg.coef_\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "importance_df['Absolute Coefficient'] = importance_df['Coefficient'].abs()\n",
    "importance_df = importance_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming best_forest_reg is the trained Random Forest model\n",
    "# importances = best_forest_reg.feature_importances_\n",
    "\n",
    "# # Create a DataFrame to display feature importance\n",
    "# importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "# importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "# print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming lasso_reg is the trained model\n",
    "# coefficients = lasso_reg.coef_\n",
    "\n",
    "# # Create a DataFrame to display feature importance\n",
    "# importance_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "# importance_df = importance_df[importance_df['Coefficient'] != 0]\n",
    "# importance_df['Absolute Coefficient'] = importance_df['Coefficient'].abs()\n",
    "# importance_df = importance_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "# print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
